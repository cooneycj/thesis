
\section{Networks}

Approaching neuroscience from the point of view that there are
clearly connections between neurons in the same region of the brain,
that these connections could possibly illuminate the functional landscape of the
 brain.  This suggested that Complex Network Theory, a branch 
of Mathematics closely related to Graph Theory, could be useful. Complex Network Theory differs from 
Graph Theory in that it focuses on the traits that networks tend to have when they evolve in real life, rather than focusing on purely random networks.  
The main difference between these would be the connectivity profile of the 
network; in Graph Theory connections are usually spread somewhat uniformly 
through the graph, whereas in Network Theory it is  expected that communities would form \cite{Newman2010a}.

A mathematical network is simply a collection of nodes and links between
these nodes.  An \emph{undirected network} with $n$ nodes can be
completely described by the adjacency matrix $A_{ij}$ where $A_{ij} =
1$ if nodes $i$ and $j$ are connected, and is equal to zero otherwise. In this 
case $A_{ij} = A_{ji}$ since two nodes are either connected or
not.  If matrix entries $A_{ij}$ are permitted to take values other than one and zero
then that is called a \emph{weighted} network, and if the matrix is not symmetric; it is called a \emph{directed} network, where $A_{ij} = 1$ if there 
is an arrow pointing from node $j$ to node $i$.   Binary undirected networks form the bulk of the literature, and so the theory is richer
in this case; often one can say that two objects, or properties, are 
related or that they are not, which further .

A particularly interesting part of Network Theory that could be useful for
Neuroscience is clustering, and algorithms to maximise the community structure of the communities.  The main focus is on a measure known as the
\emph{modularity}.

\subsection{Modularity}

The \emph{modularity} is a measure of a given clustering of a network,  it is defined to measure how much more prominent
intra-cluster links are than inter-cluster links in a given
clustering. The modularity then, as a measure of the given clustering, measures how much community structure is seen in the network.  

Thus, the modularity is a measure of a clustering of a network that assesses 
how much more community structure there is in the network than there would be 
in a random network with similar properties, such as degree of nodes.  If the 
probability of a link between nodes $i$ and $j$ is $P_{ij}$ and $g_i$ is the 
community to which node $i$ belongs, then it can be described as 
\cite{Newman2006b}:

\begin{equation}
Q = \frac{1}{2m}\sum_{i,j}[A_{ij} - P_{ij}]\delta(g_i,g_j)
\end{equation}
where $\delta$ is the Kronecker delta, so $\delta(g_i,g_j)=1$ if nodes 
$i$ and $j$ are in the same cluster, and zero otherwise. $m$ is the total 
number of links in the network, and so $\frac{1}{2m}$ is just a normalising 
factor.  The modularity of a cluster is then the number of edges within a 
cluster minus the expected number of edges.

It needs to be understood what exactly $P_{ij}$ should be.  In undirected networks, 
then clearly $P_{ij} = P_{ji}$.  The case of all nodes being in the same 
cluster is not interesting, as it displays no further community structure, so the modularity is set 
$Q=0$ in that case.  Therefore, get $\sum_{i,j}[A_{ij} - P_{ij}] = 0$, and,

\begin{equation}
\sum_{i,j}P_{ij} = \sum_{i,j} A_{ij} = 2m,
\end{equation}
where $m$, as before, is the total number of edges in the network, so if the 
degree of node $i$ is $k_i$, then

\begin{equation}
m = \frac{1}{2}\sum_i k_i = \frac{1}{2}\sum_{i,j} A_{ij}.
\end{equation}

Beyond this, there is more scope for choice.  One could suppose that each node 
in the ``random'' network has degree equal to the average degree of the 
network, but this ignores local structure in the network.  If, instead, it is supposed that 
the ``random'' network should keep the same degree for all nodes instead then the definition is derived by Newman in \cite{Newman2006a}.  So, if the expected 
degree of each vertex is equal to the actual degree of the vertex, then:

\begin{equation}
\sum_j P_{ij} = k_i.
\label{probki}
\end{equation}

If it is supposed that beyond the constrained degree distribution that edges are placed at random, 
then the probability of two nodes connecting is dependent on just their 
degrees. Supposing the probabilities for each end of a single edge is independent, a reasonable assumption in a large network {\bf[CITATION]}, where all the degrees are small 
relative to the total number of edges, get that $P_{ij} = f(k_i)f(k_j)$ for 
some function $f$ on their degrees.  Then, by equation \ref{probki}, get
\begin{equation}
\sum_{j=1}^{n}P_{ij} = f(k_i)\sum_{j=1}^nf(k_j)=k_i,
\end{equation}
so $f(k_i) = Ck_i$, for some constant $C$, and we get
\begin{equation}
2m = \sum_{i,j}P_{ij} = C^2\sum_{i,j}k_ik_j = (2mC)^2,
\end{equation}
so, $C = 1/\sqrt{2m}$ which gives the probability $P_{ij}$ as
\begin{equation}
P_{ij} = \frac{k_i k_j}{2m},
\end{equation}
and the modularity is
\begin{equation}\label{NewMod}
Q= \frac{1}{2m}\sum_{i,j} \left(A_{ij} -
\frac{k_ik_j}{2m}\right)\delta(g_i,g_j).
\end{equation}

The modularity gives values between $-1/2$ and $1$ for any clustering.  Since it is known that not 
dividing the network into separate clusters gives a modularity of zero, this 
gives us a lower bound for ``good'' clusterings.

The benefit of finding a good clustering can be seen in {\bf[FIGURE]}; 
a good clustering can reveal what the nature of the network is, rather than it 
being a mess of nodes and links.  While the modularity itself is a measure of a 
given clustering of a network, the maximum modularity is a property of the 
network itself, which can tell much about the inherent community structure of the 
network. To actually determine what clustering will give the maximum modularity an exhaustive search would be required, which becomes unfeasible for networks of 
a moderate to large size, and so clustering algorithms are needed to 
maximise the modularity.

%\begin{figure}[t]
%  \centering
%  \includegraphics[width=\textwidth]{networkcvsnc.eps}
%  \rule{35em}{0.5pt}
%  \caption{The advantage to clustering a network correctly is seen here.  Both 
%    of these networks have the same nodes and links, and so are the same 
%    network, but they look vastly different because the diagram on the left 
%    has been clustered to maximise modularity.}
%  \label{netclus}
%\end{figure}


\subsection{Newman's Eigenvalue Algorithm}

In \cite{Newman2006a} Newman noted that if it was supposed that the network was 
split into two clusters, then the modularity could be redefined in terms of a
quadratic form.  Defining a vector $s$, which keeps track of the
split,
\begin{equation}
s_i = \left\{ \begin{array}{ll} 1 & \mbox{if node $i$ in cluster 1}
  \\ -1 & \mbox{if node $i$ in cluster 2}\end{array} \right.
\end{equation}

These vectors $s_i$ then contain the exact same information as the $n\times 2$ matrix $\delta (g_i,g_j)$, so $\delta (g_i,g_j) $ can be factored by:
\begin{equation}
\delta (g_i,g_j) = \frac{1}{2} \left( s_i s_j + 1 \right)
\end{equation}
Then, the modularity is redefined as:
\begin{eqnarray} \label{ssMod}
Q & = & \frac{1}{2m}\sum_{i,j} \left(A_{ij} - \frac{k_ik_j}{2m}\right)\frac{1}{2}(s_is_j + 1)\cr
& = &\frac{1}{4m}\sum_{i,j} \left(A_{ij} - \frac{k_ik_j}{2m}\right)s_is_j,
\end{eqnarray}
Let $\mathbf{B}$ be a matrix where 
$B_{ij} = A_{ij} - k_ik_j/2m$, called the \emph{modularity matrix}, 
then equation \ref{ssMod} becomes:
\begin{equation}
Q = \frac{1}{4m}{\bf s^tBs}
\end{equation}

Now, since $\mathbf{B}$ is an $n\times n$ symmetric matrix, it has $n$ real eigenvalues $\lambda_i$, with corresponding
eigenvectors $u_i$.  Ordering the $\lambda_i$ so that $
\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$, then write 
\begin{equation}
{\bf s} = a_1{\bf u_1}+ \ldots + a_n{\bf u_n}.
\end{equation}


The equation for the modularity becomes
\begin{eqnarray}
Q & = & \frac{1}{4m} \sum_{i} a_i{\bf u}_i^t{\bf B}\sum_ja_j{\bf u}_j\cr
& = & \frac{1}{4m} \sum_i \lambda_i ({\bf u}_i^{\bf t}.{\bf s})^2
\end{eqnarray}
so, the positivity of the modularity depends completely on the $\lambda_i$, in 
particular the most positive eigenvalue $\lambda_1$.  This means that to 
maximise the modularity the ``split vector'' $\mathbf{s}$ ought to be 
as close to the first eigenvector $\mathbf{u}_1$ as possible. To do this, 
choose $\mathbf{s}$ as follows:

\begin{equation}
s_i =\left\{ \begin{array}{ll} 1 & \mbox{if } u_{1_i}>0 \\
-1 & \mbox{if } u_{1_i}\leq0 \end{array} \right.
\end{equation}

This gives a clustering of the network into two smaller clusters. Of course, 
this split is not strictly in the direction of $\lambda_1$, but it is the best 
possible estimate\footnote{In fact, it has been noted from well-known networks, 
such as Zachary's karate club \cite{Zachary1977a} that the absolute value of 
the $i$th entry of $\mathbf{u}_1$ gives a good indication of the ``strength'' 
of the membership of node $i$ to its cluster \cite{Newman2006a}.}, given two 
clusters. If the modularity of the split is not positive, then reject the 
split and say that the best clustering is to leave the network as it is.  
Similarly, if the first eigenvalue $\lambda_1 = 0$ then say that the best 
clustering is to leave the network undivided, as it is known that the modularity 
matrix $\mathbf{B}$ always has a zero eigenvalue, with eigenvector 
$\mathbf{u} = ( 1,1, \ldots, 1)$, which corresponds to no split of the network.

With the network split into two smaller clusters, the next
question is how to split the network further.  Newman \cite{Newman2006a} recommends looking at the clusters
one-by-one and splitting them until it gives no benefit to the modularity
of the overall clustering.  One could naively just look at the
adjacency matrix of the cluster itself, and form the 
corresponding modularity matrix, but this ignores the connectivity of the
overall network.  That method could inadvertantly split the subgraph into communities 
which would make sense within the cluster, but would ignore the connections 
from outside the cluster. Instead, the original modularity 
matrix $\mathbf{B}$ is used to calculate what difference a split of the subgraph would 
make to the modularity.

Call the cluster of nodes $g$ and the \emph{modularity contribution} $\Delta Q$ of a 
split of $g$ is:

\begin{equation}
\Delta Q  =  \frac{1}{2m} \left[ \frac{1}{2}\sum_{i,j\in g}B_{ij}(s_is_j + 1) - \sum_{i,j \in g} B_{ij} \right].
\end{equation}

This is the term in the modularity of the network that a split $\mathbf{s}$ of 
$g$ would change, minus the modularity of leaving the subgraph $g$ whole.  Thus, a ``subgraph modularity matrix'' $\mathbf{B}^{(g)}$ can be defined:

\begin{samepage}
\begin{eqnarray}
\Delta Q & = & \frac{1}{4m}\left[ \sum_{i,j \in g} B_{ij}s_is_j - \sum_{i,j \in g}B_{ij}\right] \cr
& = & \frac{1}{4m}\sum_{i,j \in g}\left[ B_{ij} - \delta_{ij}\sum_{k \in g} B_{ik}\right]s_is_j \cr
& = & \frac{1}{4m}{\bf s^tB}^{(g)}{\bf s}
\end{eqnarray}
\end{samepage}
and so ${\bf B}^{(g)}$ that takes the value
\begin{equation}
B^{(g)}_{ij} = B_{ij} - \delta_{ij}\sum_{k \in g} B_{ik}
\end{equation}
for labels $i,j$ of nodes in the cluster $g$.

Using the same approach as before and find the most
positive eigenvalue of ${\bf B}^{(g)}$ to determine the favoured split
${\bf s}$ to maximise $\Delta Q$. ${\bf B}^{(g)}$ still has the
property that each of its rows and sum to zero, so there still is a
zero-eigenvalue that represents no split of the cluster.  This can
now be the stopping criterion for the algorithm; if the most positive eigenvalue is the
zero-eigenvalue say that the subgraph is indivisible and stop
splitting.  However, it should be noted that as the most
positive eigenvalue gets smaller, relative to its negative
eigenvalues, there may be no benefit to the modularity of accepting
such a split.  In this case  check, by calculating $\Delta Q$ for
the proposed split, to see if it is worth continuing splitting the subgraph.
If $\Delta Q \leq 0$ then stop, otherwise continue
splitting the graph.  The algorithm is summarised in algorithm \ref{algo-split}.

\begin{algorithm}
\caption{This is Newman's eigenvalue algorithm for maximising the modularity of a network.}
\label{algo-split}
\begin{algorithmic}
\STATE Calculate the modularity matrix $\mathbf{B}$, where $B_{ij} = A_{ij} - k_ik_j/2m$
\STATE Find the most positive eigenvalue $\lambda_1$ and its eigenvector $\mathbf{u}_1$
\IF{$\lambda_1 = 0$}
\STATE Stop the algorithm with no split in the network.
\ELSE
\STATE Split network such that node $i$ in first group if $u_{1_{i}} > 0$ and in second group otherwise.
\ENSURE Modularity of split $> 0$
\ENDIF
\FORALL{Subnetworks $g$}
\STATE Calculate $\mathbf{B}^{(g)}$, where $B^{(g)}_{ij} = B_{ij} - \delta_{ij}\sum_{k\in g}B_{ik}$
\STATE Find the most positive eigenvalue $\lambda^{(g)}_1$ of $\mathbf{B}^{(g)}$ and its eigenvector $\mathbf{u}^{(g)}_1$.
\IF{$\lambda^{(g)}_1=0$}
\STATE Subnetwork can split no further.
\ELSE
\STATE Split network such that node $i$ in first group if $u^{(g)}_{1_{i}} > 0$ and in second group otherwise
\ENSURE $\Delta Q > 0$ for split of subnetwork $g$.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}



This spectral method for maximising the modularity has shown
remarkably good results \cite{Newman2006a}, in fact considerably better results
than the {\sl betweenness} algorithm of Newman and Girvan 
\cite{NewmanGirvan2004a}, despite the apparent drawback of always splitting the 
network/subgraph in two parts. It is also possible to use the first $m$ 
eigenvalues and use the $i$th entry of the eigenvectors as coordinates in 
$\mathbf{R}^{m}$ \cite{Humphries2011a}, but this then requires a choice of 
number of clusters, which may not be known from the data.  This may also 
require calculating many more eigenvalues of the large matrix $\mathbf{B}$, 
which can be computationally expensive.

\subsection{New network clustering algorithms}
Using the definition of modularity as defined above in equation \ref{NewMod}, two new algorithms were developed in the course of this thesis.  They both use complexity algorithms, and so tend to be a lot slower than principled clustering algorithms such as in \cite{Newman2006a} \cite{NewmanGirvan2004a}.

\subsubsection{Genetic algorithm}
Genetic algorithms have become very popular to solve complex questions where it is difficult to calculate the gradient of the objective function.  The algorithm is based on the process of Darwinian evolution where successful genes remain in the gene pool and unsuccessful genes die out.  There have been several clustering algorithms proposed which use the genetic algorithm as their framework, such as \cite{Pizzuti2008a}. In this proposed algorithm the genes are simply the clusterings of a given network and the modularity is then the objective function.

The simplicity of a typical genetic algorithm is that it typically only needs a fitness/objective function to evaluate the fitness of solutions.  A typical genetic algorithm is described in algorithm \ref{genal}.

\begin{algorithm}
\caption{An example of a generic genetic algorithm.}
\label{genal}
\begin{algorithmic}
\STATE Generate a random population of $n$ genes $g_i$, set generation to zero.
\WHILE{ Generation $<N$ }
\FORALL{ Genes $g_i$}
\STATE Calculate the fitness of $g_i$.
\ENDFOR
\STATE Breed new population:
\FOR{ $i \in \{ 1,\ldots,n\}$.}
\STATE Choose parents according to goodness-of-fit.
\STATE Generate new gene $\tilde{g_i}$ by crossover of parent genes.
\STATE Mutate $\tilde{g_i}$ with probability $\epsilon$.
\ENDFOR
\FORALL{$g_i$} \STATE $g_i \leftarrow \tilde{g_i}$ \ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Thus, to define a genetic algorithm, it is important to understand what a solution is, and how the genes crossover and mutate from generation to generation.  A fitness function in genetic algorithms should typically be non-negative, which can be attained by setting the fitness of any clustering with negative modularity to zero, but since the lower bound is sharp it is better to use the modularity plus $1/2$ as the fitness function, $f$. Then a clustering $g_i$ is chosen as a parent with probability $f(g_i)/(\sum_j f(g_j))$.

It is important to maintain aspects of the ``parent'' genes when breeding the new genes; the number of clusters is a very important aspect of a clustering, so any algorithm must be careful not to simply crossover clusterings by taking their intersection as that would typically increase the number of clusters.  This can be achieved by setting the upper bound for the number of clusters to be the maximum of the numbers of clusters of the parents, or by setting $n_c = \left(n_i f(g_i)+n_j f(g_j)\right)/(f(g_i)+f(g_j))$.

Crossover is defined in this algorithm by randomly choosing a single node in the network according to the degree distribution, and then randomly choose the cluster which contains the node in one of the parents.  Remove all nodes in the chosen cluster from the selection pool, and continue the process.  If the number of clusters reaches the predefined maximum number of clusters and some nodes remain, then the remaining nodes are placed in the closest cluster to them (by path distance).  Mutation is defined by randomly moving a small (~$15\%$) percentage of nodes between clusters in approximately $10\%$ of clusterings.

This algorithm is very computationally expensive for small networks, but scales much better than Newman's eigenvalue algorithm.

\subsubsection{Simulated Annealing}

Another algorithm from complexity theory which has previously been used for finding community structure in networks is \emph{simulated annealing}.  Simulated annealing is an algorithm based on the technique of annealing in metallurgy; annealing happens when a metal is heated and then cooled slowly to increase the size of the crystals in the metal, as the metal finds the lowest energy state as it cools slowly.

Simulated annealing mimics this process to find a solution that minimises an \emph{energy} function, by introducing a \emph{temperature} of sorts to the system. The probability of changing the state of the system is then determined by an \emph{acceptance probability function}, $P(E(s),E(s'),T)$, which depends on the current energy $E(s)$, the potential new energy $E(s')$ and the temperature $T$.  The basic rule of the algorithm is that any change of state that reduces the energy of the system is automatically accepted, and a change of state that increases the energy is accepted with a probability that decreases as $T\rightarrow0$.

The acceptance probability function used also has the property that small increases in energy are more likely to be accepted than big increases in energy, as can be seen:
\begin{equation}
P(E(s),E(s'),T) = \left\{ \begin{array}{ll} 1 & E(s')<E(s) \\ min(e^{-\frac{E(s')-E(s)}{T}},1) & E(s') \geq E(s) \end{array} \right.
\end{equation}





\subsection{$k$-Medoids Clustering}

By defining a metric on spike trains, we can tell how close different responses 
are to each other, but there are no ``co-ordinates'' in the metric space which 
would help with clustering responses.  The standard method of clustering 
spike-trains is similar to $k$-means clustering, but since there is no standard 
method for computing a ``mean spike-train'' given a number of spike trains, it 
must be altered slightly.

For $k$-means clustering, we select $k$ random points in the space in which 
clustering is occuring, then each point in the space would be in the cluster of 
the closest of these $k$ points to itself.  Then we would find the mean of all 
the points in a cluster to get a new ``centre'' for the cluster and re-cluster 
each point to the closest of these $k$ means.  This process is repeated until 
the means are stable, which gives a clustering of the points into $k$ clusters.

Unfortunately, since there is no mean in the metric space of spike-trains, we 
cannot perform $k$-means clustering, rather, we perform $k$-medoids.  In 
$k$-medoids, we randomly choose $k$ points in the data, which we call medoids, 
and then we place each other point of the data in the cluster of the 
``closest'' (or, least dissimilar) medoid to it.  Then, for each cluster, we 
swap the medoid for each other point in the cluster, and choose the one with 
the least overall distance as the new medoid. This step is shown in Figure 
\ref{kmed} below. We repeat these steps until there is no change in the 
medoids.  This method suits us for spike-trains, as the idea of a ``mean'' 
spike-train is not well understood, but there is currently a very promising 
method proposed by other members of this lab, which is to be submitted shortly.

There are some downsides to using $k$-medoids, that I hoped could possibly be 
improved by using network methods.  The primary disadvantage is that you must 
select how many clusters in advance, and hence you must know how many different 
stimuli were used when you look at a data set.  The next chapter deals with our 
efforts to address this issue.

%\begin{figure}[t]
%  \centering
%  \includegraphics[width=0.8\textwidth]{kmedoids.eps}
%  \rule{35em}{0.5pt}
%  \caption{An example of the first step in k-medoids clustering:  (a) Medoids 
%    are initially selected at random and the first clustering puts each point 
%    in the cluster of the closest medoid. Then we choose the point in the 
%    cluster that will give the lowest total distance to other points, and 
%    choose that as the new medoid. (b) Finally, we re-cluster each point to go 
%    in the cluster of the closest medoid to it.}
%  \label{kmed}
%\end{figure}

\subsection{Clustering Responses with Modularity}


We proposed that we could perhaps improve on current methods of sorting 
responses by using the modularity clustering algorithm.  The advantage to such 
a method would be that we would hopefully not need to know how many different 
stimuli there were. Since the algorithm determines when to stop itself, it 
could simply be run to completion.  Such a method could perhaps be used for 
many different data sets, when the number of stimuli was not known, to 
determine the different responses.

We used the data set from \cite{NarayanEtAl2006b}, which featured spike trains 
from anesthetized adult male zebra finches as they listened to natural 
birdsong.  Each neuron was played $20$ songs ten times each, so we had $200$ 
data points from which to form a network for each neuron.

We formed a network by first taking the Van-Rossum metric distances between 
each pair of responses, then we chose a threshold value $\tau$ for the network 
and we set

\begin{equation}
A^{\tau}_{ij} = \left\{ \begin{array}{ll} 1 & \mbox{if }d(i,j)<\tau \\
0 & \mbox{otherwise}
\end{array}\right. .
\end{equation}

Once we have the adjacency matrix for our network, we can run the algorithm to 
maximise modularity for the network.  For the data that we used, 
each spike train was less than a second long, so the maximum van Rossum 
distance between any two trains was one, so we ran the algorithm for different 
threshold values $\tau$ between zero and one incrementing $\tau$ by steps of 
size $0.001$.

As we can see in Figure \ref{graphs}, the profile of the graph of the maximum 
modularity versus the threshold looked promising, as in nearly all cases it had 
a clear maximum, but unfortunately the clusters of responses bore little 
resemblance to the stimuli, and usually had too few clusters.

%\begin{figure}[thb]
%  \centering
%  \includegraphics[width=0.8\textwidth]{expresults.eps}
%  \rule{35em}{0.5pt}
%  \caption{Above you can see the graph of the modularity versus threshold values
%    in two typical cells from the data.  The green crosses represent the 
%    modularity of the correct clustering of stimuli; the red plusses represent 
%    the maximum modularity, according to the algorithm, of the network.
%    Despite there being clear peaks in the graphs, we see that the green 
%    consistently outperforms the red for every value of the threshold $\tau$.  
%    This seems to imply that network clustering methods are probably not very 
%    good for sorting stimuli.}
%  \label{graphs}
%\end{figure}

It appears that the margins are just too small, and there is no natural 
network, rather a bunch of networks determined by a strict cut-off, so the 
modularity algorithm cuts some of the correct clusters in two.  A recent paper 
by Humphries \cite{Humphries2011a} uses all of the positive eigenvalues to 
cluster the responses with more accuracy. This method is 
rather computationally expensive, and uses even the very small positive 
eigenvalues, which tell us very little about the positivity of modularity, so 
we tried to use Newman's more simple algorithm, despite its obvious drawbacks.

The problem with the above method appears to be that there is no natural network formed by the distance matrix of responses to stimuli.






\section{Discussion}
Due to the fact that the modularity that the algorithm found for the network is 
higher than the modularity of the correct clustering, it is clear that network 
methods are not useful to sort stimuli from a single neuron.  Since the 
original goal was to illuminate the functionality of the brain, or at least 
small areas of the brain, this non-result doesn't matter too much.  We want to 
get simultaneous recordings from many neurons, and form networks which would 
correspond to different neurons rather than spike trains.

There are different ways to form the networks of many neurons, but we will 
mainly be interested in neurons that seem to {\sl drive} other neurons.  That 
is, neurons whose firing seems to improve the probability of the other neuron 
firing significantly.  A method that particularly interests us is the method of 
{\sl Incremental Mutual Information} of Singh and Lesica 
\cite{SinghLesica2010a}, which tries to reduce the uncertainty of a neuron as 
much as possible before checking whether another neuron influences it.

Any network formed from this measure would have to be directed, so we need a 
method to turn a directed network into an undirected network.  Newman describes 
a very neat way to do this in \cite{Newman2010a}, where two nodes are related 
in the undirected network by how many commons nodes they point to in the 
directed network.  He calls this the {\sl bibliographic coupling} of two 
nodes.  In a brain network, this could give us a ``map'' of information flow.

Once we have our new networks, we may use some other network measures to 
examine them as in \cite{RubinovSporns2010a}.  The {\sl clustering coefficient} 
is a useful local measure for how clustered a node is, and it may be useful to 
use this along with the modularity to find and study community structure.



