
This chapter approaches neuroscience from the point of view that there are
clearly functional connections between neurons in the same region of the brain and
that these connections could provide a map of information flow in the brain.  Complex network theory, a branch 
of mathematics closely related to graph theory, is used frequently in computational neuroscience. 

Complex networks often exhibit a community structure, and so there are many algorithms to find the best clustering of a given network \citep{KarypisHanKumar2000a,NewmanGirvan2004a,Newman2006b}. This chapter introduces the clustering algorithm developed by \citet{Newman2006b}, and suggests some new algorithms based on the modularity measure \citep{NewmanGirvan2004a}.  These new algorithms are based on complexity algorithms, genetic algorithm and simulated annealing, which require nothing more than a \lq{}goodness-of-fit\rq{} function.  Complexity algorithms often produce interesting results, as they do not take a pre-determined path through the solution space.

Newman's algorithm \citep{Newman2006b} is then used to cluster neuron responses dynamically, where the number of stimuli need not be known. A network of responses is formed by comparing the van Rossum distances between the spike trains, and then choosing a distance threshold where any distance less than the threshold forms a link.  By clustering this network using the eigenvalue algorithm of \citet{Newman2006a}, it is not necessary to specify the number of stimuli in advance, unlike the commonly used methods, $k$-means and $k$-medoids.


\section{Introduction to networks}
Complex network theory differs from 
graph theory in that it focuses on the traits that networks tend to have when they evolve in real life, rather than focusing on purely random networks.  
The main difference between these would be the connectivity profile of the 
network; in graph theory connections are usually spread somewhat uniformly 
throughout the graph, whereas in Network Theory it is  expected that communities would form \citep{Newman2010a}.

A mathematical network is simply a collection of nodes and links between
these nodes.  A \emph{node} is simply a point in the graph, which is typically discrete and labelled. A \emph{link} is a pair of nodes which indicates a connection between the two nodes.  The \emph{degree} of a node is defined to be the number of links incident to the node.  An \emph{undirected network} with $n$ nodes can be
completely described by its adjacency matrix $\mathbf{A}$ where $A_{ij} =
1$ if nodes $i$ and $j$ are connected, and is equal to zero otherwise. In this 
case $A_{ij} = A_{ji}$ since two nodes are either connected or
not.  If matrix entries $A_{ij}$ are permitted to take values other than one and zero
then that is called a \emph{weighted} network. A network is called a \emph{directed} network if $A_{ij} = 1$ whenever there 
is an arrow pointing from node $j$ to node $i$, and the adjacency matrix is not  necessarily symmetric.   Undirected networks which are unweighted form the bulk of the literature, and so the theory is richer
in this case. A reason for this could be that networks such as friend networks \citep{Zachary1977a} would typically not be directed as friendship is typically mutual.

A particularly interesting part of network theory that could be useful for
neuroscience is clustering, and algorithms to maximise the community structure of the clusterings. To maximise the community structure, a measure of such structure is required.

\section{Modularity}

The \emph{modularity} is a measure of a given clustering of a network, which gives a value to the community structure of the clustering \citep{NewmanGirvan2004a}. The modularity is defined to measure how much more prominent
intra-cluster links are than inter-cluster links in a given
clustering. The modularity then, as a measure of the given clustering, measures how much community structure is seen in the network.  It was introduced by \citet{NewmanGirvan2004a} in one form, then \citet{Newman2006b, Newman2006a} derived another form using the configuration model for graphs \citep{BenderCanfield1978a,Bollobas1980a}.


\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{networkcvsnc.eps}
  \bigskip
  \rule{31.5em}{0.5pt}
  \caption{The advantage to clustering a network correctly can be seen here.  Shown  
    are two diagrams which have the same nodes and links, and so form the same 
    network, but they look vastly different because the diagram on the left 
    has been clustered to maximise modularity.}
  \label{netclus}
\end{figure}

The modularity is a measure of a clustering of a network that assesses 
how much more community structure there is in the network than there would be 
in a random network with similar properties.  In \citep{Newman2006b,Newman2006a} the modularity is derived as follows in this section. The modularity is defined to be the difference between actual links within clusters and the expected number of links.  Summing over all nodes within a cluster, $g$:
\begin{equation}
(\text{Links in }g) - (\text{Expected links in }g) = \sum_{i,j \in g}  A_{ij} - \sum_{i,j \in g} P_{ij} 
\end{equation}
where $P_{ij}$ is the 
probability of a link between nodes $i$ and $j$. Then, if $g_i$ is the 
community to which node $i$ belongs, the modularity $Q$ can be described:

\begin{equation}
Q = \frac{1}{2m}\sum_{i,j}[A_{ij} - P_{ij}]\delta(g_i,g_j)
\end{equation}
where $\delta$ is the Kronecker delta, so $\delta(g_i,g_j)=1$ if nodes 
$i$ and $j$ are in the same cluster, and zero otherwise. $m$ is the total 
number of links in the network, and so $1/2m$ is just a normalising 
factor.  The modularity of a cluster is then the number of edges within a 
cluster minus the expected number of edges.

\citet{Newman2006b,Newman2006a} calculates the probability of a link between nodes in the configuration model.  In undirected networks, 
clearly the probabilities should be symmetric, $P_{ij} = P_{ji}$.  Then, the expected number of links across the whole network should equal the total number of links.   That is, $\sum_{i,j}[A_{ij} - P_{ij}] = 0$, and,
\begin{equation}
\sum_{i,j}P_{ij} = \sum_{i,j} A_{ij} = 2m,
\end{equation}
where $m$, as before, is the total number of edges in the network, so if the 
degree of node $i$ is $k_i$, then
\begin{equation}
m = \frac{1}{2}\sum_i k_i = \frac{1}{2}\sum_{i,j} A_{ij}.
\end{equation}

At this point, there is some choice as to how similar the random network, that the calculated probabilities are based on, should be to the network itself.  One could suppose that each node 
in the random network has degree equal to the average degree of the 
network, but this ignores local structure in the network.  Instead, it is supposed that 
the random network should keep the same degree for all nodes, and so the network can be treated as an instance of the \emph{configuration model} \citep{BenderCanfield1978a, Bollobas1980a}.  So, the expected 
degree of each vertex is equal to the actual degree of the vertex:
\begin{equation}
\sum_j P_{ij} = k_i.
\label{probki}
\end{equation}

If it is supposed that beyond the degree distribution the edges are placed at random, 
then the probability of two nodes connecting depends on their 
degrees. Supposing the probabilities for each end of a single edge is independent, which is a reasonable assumption in a large network \citep{Newman2010a}, where all the degrees are small 
relative to the total number of edges, $P_{ij} = f(k_i)f(k_j)$ for 
some function $f$ on their degrees.  Then, by equation \ref{probki}:
\begin{equation}
\sum_{j=1}^{n}P_{ij} = \sum_{j=1}^n f(k_i)f(k_j) = f(k_i)\left[\sum_{j=1}^n f(k_j)\right]=k_i,
\end{equation}
so $f(k_i) = Ck_i$, for some constant $C$, and we get
\begin{equation}
2m = \sum_{i,j}P_{ij} = C^2\sum_{i,j}k_ik_j = (2mC)^2,
\end{equation}
so, $C = 1/\sqrt{2m}$ which gives the probability $P_{ij}$ as
\begin{equation}
P_{ij} = \frac{k_i k_j}{2m},
\end{equation}
and the modularity is
\begin{equation}\label{NewMod}
Q= \frac{1}{2m}\sum_{i,j} \left(A_{ij} -
\frac{k_ik_j}{2m}\right)\delta(g_i,g_j).
\end{equation}

The modularity gives values between $-1/2$ and $1$ for any clustering.  Since it is known that not 
dividing the network into separate clusters gives a modularity of zero, this 
gives us a lower bound for ``good'' clusterings.

The benefit of finding a good clustering can be seen in figure \ref{netclus}; 
a good clustering can reveal what the nature of the network is, rather than it 
being a mess of nodes and links.  While the modularity itself is a measure of a 
given clustering of a network, the maximum modularity is a property of the 
network itself, which can tell much about the inherent community structure of the 
network. To actually determine what clustering will give the maximum modularity an exhaustive search would be required, which becomes unfeasible for networks of even  a moderate size, as the number of potential clusterings is huge. Therefore, clustering algorithms are needed to 
maximise the modularity of the network.

\section{Newman's eigenvalue algorithm}

In \citep{Newman2006a,Newman2006b} Newman noted that if it was supposed that the network was 
split into two clusters, then the modularity could be redefined in terms of a
quadratic form.  Defining a vector $s$, which keeps track of the
split,
\begin{equation}
s_i = \left\{ \begin{array}{ll} 1 & \mbox{if node $i$ in cluster 1}
  \\ -1 & \mbox{if node $i$ in cluster 2}\end{array} \right.
\end{equation}
These vectors $s_i$ then contain the exact same information as the $2\times n$ matrix $\delta (g_i,g_j)$, so $\delta (g_i,g_j) $ can be factored:
\begin{equation}
\delta (g_i,g_j) = \frac{1}{2} \left( s_i s_j + 1 \right)
\end{equation}
Then, the modularity is redefined as:
\begin{eqnarray} \label{ssMod}
Q & = & \frac{1}{2m}\sum_{i,j} \left(A_{ij} - \frac{k_ik_j}{2m}\right)\frac{1}{2}(s_is_j + 1)\cr
& = &\frac{1}{4m}\sum_{i,j} \left(A_{ij} - \frac{k_ik_j}{2m}\right)s_is_j,
\end{eqnarray}
Let $\mathbf{B}$ be a matrix where 
$B_{ij} = A_{ij} - k_ik_j/2m$, called the \emph{modularity matrix}, 
then equation \ref{ssMod} becomes:
\begin{equation}
Q = \frac{1}{4m}{\bf s^tBs}
\end{equation}

Now, since $\mathbf{B}$ is an $n\times n$ symmetric matrix, it has $n$ real eigenvalues $\lambda_i$, with corresponding
eigenvectors $u_i$.  Ordering the $\lambda_i$ so that $
\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$, then write 
\begin{equation}
{\bf s} = a_1{\bf u_1}+ \ldots + a_n{\bf u_n}.
\end{equation}


The equation for the modularity becomes
\begin{eqnarray}
Q & = & \frac{1}{4m} \sum_{i} a_i{\bf u}_i^t{\bf B}\sum_ja_j{\bf u}_j\cr
& = & \frac{1}{4m} \sum_i \lambda_i ({\bf u}_i^{\bf t}.{\bf s})^2
\end{eqnarray}
so, the positivity of the modularity depends completely on the $\lambda_i$, in 
particular the most positive eigenvalue $\lambda_1$.  This means that to 
maximise the modularity the ``split vector'' $\mathbf{s}$ ought to be 
as close to the first eigenvector $\mathbf{u}_1$ as possible. To do this, 
choose $\mathbf{s}$ as follows:

\begin{equation}
s_i =\left\{ \begin{array}{ll} 1 & \mbox{if } u_{1_i}>0 \\
-1 & \mbox{if } u_{1_i}\leq0 \end{array} \right.
\end{equation}

This gives a clustering of the network into two smaller clusters. Of course, 
this split is not strictly in the direction of $\lambda_1$, but it is the best 
possible estimate, given two 
clusters. In fact, it has been noted from well-known networks, 
such as Zachary's karate club \citep{Zachary1977a}, where friendships within a karate club were graphed prior to the club splitting, that the absolute value of 
the $i$th entry of $\mathbf{u}_1$ gives a good indication of the ``strength'' 
of the membership of node $i$ to its cluster \citep{Newman2006a}. That is, an entry $u_1,i$ with absolute value close to one can be observed to be a key intra-cluster node or hub.  In the case of Zachary's karate club these intra-cluster hubs were the different instructors who split the club into two clubs.

If the modularity of the split is not positive, then reject the 
split and say that the best clustering is to leave the network as it is.  
Similarly, if the first eigenvalue $\lambda_1 = 0$ then say that the best 
clustering is to leave the network undivided, as it is known that the modularity 
matrix $\mathbf{B}$ always has a zero eigenvalue, with eigenvector 
$\mathbf{u} = ( 1,1, \ldots, 1)$, which corresponds to no split of the network.

With the network split into two smaller clusters, the next
question is how to split the network further.  Newman \citep{Newman2006a} recommends looking at the clusters
one-by-one and splitting them until it gives no benefit to the modularity
of the overall clustering.  One could naively just look at the
adjacency matrix of the cluster itself, and form the 
corresponding modularity matrix, but this ignores the connectivity of the
overall network.  That method could inadvertently split the subgraph into communities 
which would make sense within the cluster, but would ignore the connections 
from outside the cluster. Instead, the original modularity 
matrix $\mathbf{B}$ is used to calculate what difference a split of the subgraph would 
make to the modularity.

Given a cluster of nodes $g$, within the network, the \emph{modularity contribution} $\Delta Q$ of a 
split of $g$ is:

\begin{equation}
\Delta Q  =  \frac{1}{2m} \left[ \frac{1}{2}\sum_{i,j\in g}B_{ij}(s_is_j + 1) - \sum_{i,j \in g} B_{ij} \right].
\end{equation}
This is the term in the modularity of the network that a split $\mathbf{s}$ of 
$g$ would change, minus the modularity of leaving the subgraph $g$ whole.  Thus, a ``subgraph modularity matrix'' $\mathbf{B}^{(g)}$ can be defined as:
\begin{eqnarray}
\Delta Q & = & \frac{1}{4m}\left[ \sum_{i,j \in g} B_{ij}s_is_j - \sum_{i,j \in g}B_{ij}\right] \cr
& = & \frac{1}{4m}\sum_{i,j \in g}\left[ B_{ij} - \delta_{ij}\sum_{k \in g} B_{ik}\right]s_is_j \cr
& = & \frac{1}{4m}{\bf s^tB}^{(g)}{\bf s}
\end{eqnarray}
and so ${\bf B}^{(g)}$ takes the value
\begin{equation}
B^{(g)}_{ij} = B_{ij} - \delta_{ij}\sum_{k \in g} B_{ik}
\end{equation}
for labels $i,j$ of nodes in the cluster $g$.

\begin{algorithm}
\caption{This is Newman's eigenvalue algorithm for maximising the modularity of a network.}
\label{algo-split}
\begin{algorithmic}
\STATE Calculate the modularity matrix $\mathbf{B}$, where $B_{ij} = A_{ij} - k_ik_j/2m$
\STATE Find the most positive eigenvalue $\lambda_1$ and its eigenvector $\mathbf{u}_1$
\IF{$\lambda_1 = 0$}
\STATE Stop the algorithm with no split in the network.
\ELSE
\STATE Split network such that node $i$ in first group if $u_{1_{i}} > 0$ and in second group otherwise.
\ENSURE Modularity of split $> 0$
\ENDIF
\FORALL{subnetworks $g$}
\STATE Calculate $\mathbf{B}^{(g)}$, where $B^{(g)}_{ij} = B_{ij} - \delta_{ij}\sum_{k\in g}B_{ik}$
\STATE Find the most positive eigenvalue $\lambda^{(g)}_1$ of $\mathbf{B}^{(g)}$ and its eigenvector $\mathbf{u}^{(g)}_1$.
\IF{$\lambda^{(g)}_1=0$}
\STATE Subnetwork can split no further.
\ELSE
\STATE Split network such that node $i$ in first group if $u^{(g)}_{1_{i}} > 0$ and in second group otherwise
\ENSURE $\Delta Q > 0$ for split of subnetwork $g$.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The same approach as before is used, the most
positive eigenvalue of ${\bf B}^{(g)}$ determines the favoured split
${\bf s}$ to maximise $\Delta Q$. ${\bf B}^{(g)}$ still has the
property that each of its rows sum to zero, so there still is a
zero-eigenvalue that represents no split of the cluster.  This can
now be the stopping criterion for the algorithm; if the most positive eigenvalue is the
zero-eigenvalue, then that subgraph is indivisible and stop
splitting.  It should be noted, however, that as the most
positive eigenvalue gets smaller, relative to its negative
eigenvalues, there may be no benefit to the modularity of accepting
such a split.  In this case  check, by calculating $\Delta Q$ for
the proposed split, to see if it is worth continuing splitting the subgraph.
If $\Delta Q \leq 0$ then stop, otherwise continue
splitting the graph.  The algorithm is summarised in Algorithm \ref{algo-split}.  

This eigenvalue method for maximising the modularity has shown
remarkably good results \citep{Newman2006a}, despite the apparent drawback of always splitting the 
network/subgraph in two parts.  It is possible to split the network into more than two parts initially by using the first $m$ 
eigenvalues and using the $i$th entry of the eigenvectors as coordinates in 
$\mathbf{R}^{m}$ \citep{Humphries2011a}, but this then requires a choice of 
number of clusters.  This may also 
require calculating many more eigenvalues of the large matrix $\mathbf{B}$, 
which can be computationally expensive, rather than calculating a single eigenvalue for smaller and smaller matrices as above.  


\section{New network clustering algorithms}
Using the definition of modularity as defined above in equation \ref{NewMod} to measure the community structure of given clusterings, two new algorithms were developed in the course of this thesis.  They both use complexity algorithms, which can often find novel solutions to difficult problems, since they do not make assumptions about the nature of the solutions.

\subsection{Genetic algorithm}
Genetic algorithms have become widely used to solve complex optimisation problems where it is difficult to calculate the gradient of the objective function.  The algorithm is based on the process of Darwinian evolution where successful genes remain in the gene pool and unsuccessful genes die out.  There have been several clustering algorithms which use the genetic algorithm as their framework, such as \citep{Pizzuti2008a}. In this proposed algorithm the genes are simply the clusterings of a given network and the modularity is then the objective function.

The simplicity of a typical genetic algorithm is that it typically only needs a fitness/objective function to evaluate the fitness of solutions.  A genetic algorithm is described in Algorithm \ref{genal}.

\begin{algorithm}
\caption{An example of a generic genetic algorithm.}
\label{genal}
\begin{algorithmic}
\STATE Generate a random population of $n$ genes $g_i$, set generation to zero.
\WHILE{ Generation $<N$ }
\FORALL{ Genes $g_i$}
\STATE Calculate the fitness of $g_i$.
\ENDFOR
\STATE Breed new population:
\FOR{ $i \in \{ 1,\ldots,n\}$.}
\STATE Choose parents according to goodness-of-fit.
\STATE Generate new gene $\tilde{g_i}$ by crossover of parent genes.
\STATE Mutate $\tilde{g_i}$ with probability $\epsilon$.
\ENDFOR
\FORALL{$g_i$} \STATE $g_i \leftarrow \tilde{g_i}$ \ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}

To define a genetic algorithm, it is important to define what a solution is, and how the genes crossover and mutate from generation to generation. In this case, a solution is any clustering of the network.  A fitness function is a function of solutions which should typically be non-negative. This can be attained by setting the fitness of any clustering with negative modularity to zero, but since the lower bound is sharp it is better to use the modularity plus $1/2$ as the fitness function, $f$. Then a clustering $g_i$ is chosen as a parent with probability $f(g_i)/(\sum_j f(g_j))$.

It is important to maintain aspects of the ``parent'' genes when breeding the new genes; the number of clusters is a very important aspect of a clustering, so any algorithm must be careful not to simply crossover clusterings by taking their intersection as that would typically increase the number of clusters.  The upper bound for the number of clusters should be set to the maximum of the numbers of clusters of the parents, or by setting $n_c = \left(n_i f(g_i)+n_j f(g_j)\right)/(f(g_i)+f(g_j))$.

Crossover is defined in this algorithm by randomly choosing a single node in the network according to the degree distribution, and then randomly choosing the cluster which contains the node in one of the parents.  Remove all nodes in the chosen cluster from the selection pool, and continue the process.  If the number of clusters reaches the predefined maximum number of clusters and some nodes remain, then the remaining nodes are placed in the closest cluster to them (by path distance).  Mutation is defined by randomly moving a small percentage, approximately $15\%$, of nodes between clusters in approximately $10\%$ of clusterings.

This algorithm is computationally very expensive for small networks, but scales much better than Newman's eigenvalue algorithm.

\subsection{Simulated annealing}

Another algorithm from complexity theory which has previously been used for finding community structure in networks is \emph{simulated annealing}.  Simulated annealing is an algorithm based on the technique of annealing in metallurgy; annealing happens when a metal is heated and then cooled slowly to increase the size of the crystals in the metal, as the metal finds the crystal structure which requires lowest energy as it cools slowly.

Simulated annealing mimics this process to find a solution that minimises an \emph{energy} function, by introducing a \emph{temperature} of sorts to the system. At each temperature level, there is a small change to the solution, and the new energy level is calculated.

The probability of accepting the change to the solution is then determined by an \emph{acceptance probability function}, $P(E(s),E(s'),T)$, which depends on the current energy $E(s)$, the potential new energy $E(s')$ and the temperature $T$.  The basic rule of the algorithm is that any change of state that reduces the energy of the system is automatically accepted, and a change of state that increases the energy is accepted with a probability that decreases as $T\rightarrow0$.

The acceptance probability function used also has the property that small increases in energy are more likely to be accepted than big increases in energy.  The acceptance probability is then a decreasing function of the change in energy $\Delta E = E(s')-E(s)$, which decreases faster as the temperature lowers.  An exponential function satisfies these criteria:
\begin{equation}
P(E(s),E(s'),T) = \left\{ \begin{array}{ll} 1 & E(s')-E(s)<0 \\ \exp\left(-\frac{E(s')-E(s)}{T}\right) & E(s') - E(s)\geq 0 \end{array} \right.
\end{equation}

The solution space for this algorithm was the space of clusterings of a given network $N$.  The ``energy'' function was the modularity with the sign reversed.

It is important for simulated annealing that all moves are small.  With this in mind, the move in this clustering algorithm is simply to select a node at random and move it to either any existing cluster or to form a new cluster from it.  From observations, this algorithm did not appear to be very discriminating when required to split a network to more than two or three clusters, but it did seem to find ``stable'' solutions. For example, in Zachary's karate club network \citep{Zachary1977a} it found the correct split of the network, rather than the clustering which resulted in a slightly higher modularity. 

\section{$k$-medoids clustering}
By defining a metric on spike trains, there is a way to tell how \lq{}close\rq{} different responses are, but there are no co-ordinates in the metric space which 
would help with clustering responses.  Provided that the number of stimuli is known beforehand, the standard method of clustering 
spike-trains is an analog to $k$-means clustering which does not require calculating a mean solution. \citep{JulienneHoughton2012a} introduces a method for computing a \lq{}mean spike train\rq{}, so $k$-means clustering could now be used.  However, here the process of $k$-medoids clustering is described.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.6\textwidth]{kmedoids.eps}
  \bigskip
  \rule{31.5em}{0.5pt}
  \caption{An example of the first step in k-medoids clustering:  (a) Medoids 
    are initially selected at random and the first clustering puts each point 
    in the cluster of the closest medoid. Then we choose the point in the 
    cluster that will give the lowest total distance to other points, and 
    choose that as the new medoid. (b) Finally, we re-cluster each point to go 
    in the cluster of the closest medoid to it.  \label{kmed}}
\end{figure}


For $k$-means clustering, $k$ random points are selected in the space in which 
clustering is occurring, then each point in the space would be in the cluster of 
the closest of these $k$ points to itself.  Then the mean of all 
the points in a cluster is calculated to get a new ``centre'' for the cluster and re-cluster 
each point to the closest of these $k$ means.  This process is repeated until 
the means are stable, which gives a clustering of the points into $k$ clusters.

The $k$-medoids algorithm begins by choosing a pre-determined number, $k$, of points in the data, called medoids, 
and then  each other point of the data is placed in the cluster of the 
closest, or least dissimilar), medoid to it.  Then, for each cluster, 
 each point in the cluster is assessed, and the point with 
the least overall distance is chosen as the new medoid. This step is shown in Figure 
\ref{kmed}. These steps are repeated until there is no change in the 
medoids.  This method suits for spike-trains, as the idea of a `mean'
spike train is not well understood, although, using the van Rossum metric, the average function of \citet{JulienneHoughton2012a} is a possible alternative.

There are some downsides to using $k$-medoids, that could possibly be 
improved by using network methods.  The primary disadvantage is that the number of 
 clusters must be selected in advance, and hence implies prior knowledge of the number of different 
stimuli that are presented in a data set. This is addressed in the next section.

\section{Clustering responses with modularity}

Here it is proposed that current methods of sorting 
responses could potentially be improved by using the modularity clustering algorithm.  An advantage to such 
a dynamic method would be the ability to cluster responses without foreknowledge of the stimuli.  This suggests that future dynamic sorting algorithms may be possible. Since the algorithm determines when to stop itself, it is simply be run to completion.  Such a method could perhaps be used for 
many different data sets, when the number of stimuli was not known, to 
determine the different responses.

The data set introduced in the introduction from \citep{NarayanEtAl2006b} was used, which featured spike trains 
from anaesthetised adult male zebra finches as they listened to conspecific 
songs.

A distance matrix was calculated by taking the van Rossum metric distances between 
each pair of responses for each neuron, where the parameter $\tau$ in the metric was chosen to be $12.8$ ms, as this had previously been determined to be a good parameter for the data set used \citep{Houghton2009a}.    Then, given a threshold value, $\mu$, the adjacency matrix for a network was calculated as:
\begin{equation}
A^{\tau}_{ij} = \left\{ \begin{array}{ll} 1 & \mbox{if }d(i,j)<\mu \\
0 & \mbox{otherwise}
\end{array}\right. .
\end{equation}

\begin{figure}[h!tb]
  \centering
  \includegraphics[width=0.5\textwidth]{expresults.eps}
  \rule{31.5em}{0.5pt}
  \caption{Above is the graph of the modularity versus threshold values
    in two typical cells from the data.  The green crosses represent the 
    modularity of the correct clustering of stimuli; the red plusses represent 
    the maximum modularity, according to the algorithm, of the network.
      \label{graphs}}
\end{figure}

Once the adjacency matrix was calculated, the algorithm was run to 
maximise modularity for the network.  The maximum van Rossum 
distance, once normalised, between any two trains was one, so the algorithm was run for different 
threshold values $\mu$ between zero and one incrementing $\mu$ by steps of 
size $0.001$.

As seen in Figure \ref{graphs}, the profile of the graph of the maximum 
modularity versus the threshold looked promising, as for each cell the modularity peaked as $\mu$ varied, but unfortunately the clusters of responses bore little 
resemblance to the stimuli, and usually had too few clusters. Despite there being clear peaks in the modularity graphs, the maximised modularity consistently outperforms the modularity of the correct clustering for every value of the threshold $\mu$.  This seems to imply that network clustering methods are probably not very good for sorting stimuli.


The modularity maximisation of these networks unfortunately cannot cluster the responses by their stimuli. A similar method was used in \citep{Humphries2011a}, where all of the positive eigenvalues were used to 
cluster responses, . This method is 
rather computationally expensive, and uses even the very small positive 
eigenvalues, which themselves may have little effect on the positivity of modularity.  This motivated the attempt to use the clustering method of \citet{Newman2006b}, but it did not perform very well.

The problem with the above method appears to be that there is no natural network formed by the distance matrix of responses to stimuli. The next section introduces a simulated network of neurons, so that there is a definite network structure to investigate.

\section{Discussion}
In this chapter, complex networks were introduced, along with a concept of goodness-of-fit for a specific clustering in the modularity.  The eigenvalue algorithm introduced by \citet{Newman2006b} was introduced in detail, as it has become the benchmark for clustering algorithms.

Two new algorithms were introduced to try to find a clustering that would maximise the modularity of a network. Neither produced results better than the standard methods, so were used sparingly. The simulated annealing algorithm appeared to not necessarily maximise the modularity, but seemed to find the highest \lq{}stable\rq{} modularity. 

Network methods did not cluster stimuli well, perhaps because the network structure was not natural. This may rule out network clustering methods as a tool for discriminating between responses, but \citep{Humphries2011a} seemed to get results with a different data set, so it is not conclusive.  The simulated annealing algorithm introduce in this chapter could potentially provide interesting clusterings of stimuli, but that was not explored in the work for this thesis.

The dynamic response clustering appeared not to work due to a lack of network structure amongst the responses.  The next chapter thus focuses on a simulated network of neurons with known network structure.  This appears to be the optimal way to use network clustering in neuroscience.
\cleardoublepage

%
%  NEW CHAPTER
%

\chapter{Mapping information flow in a simulated network of neurons}

In this chapter, a process is proposed for a way to map how information flows through a network of neurons.  The process is proposed due to the observation that given an inter-spike interval (ISI) distribution which arises from a neuron having a refractoriness after each spike, such as the gamma function or the inverted normal distribution, then a very small group of neurons, say 20 to 30, is sufficient for the collection of neurons together to produce an exponential distribution of ISIs - which would be the ISI distribution of a Poisson rate process.  Due to this observation, it is proposed that there should be small clusters of neurons that work together to provide a background \lq{}rate\rq{} to other similar groups of neurons.

There are different ways to form the networks of many neurons, but of primary interest are neurons whose firing seems to improve the probability of other neurons 
firing.  A method of particular interest is the method of 
\emph{Incremental Mutual Information} of Singh and Lesica 
\citep{SinghLesica2010a}, which tries to reduce the uncertainty of a neuron as 
much as possible before checking whether another neuron influences it.

Any network formed from this measure would have to be directed, so a 
method to turn a directed network into an undirected network is needed.  Newman describes 
a very neat way to do this in \citep{Newman2010a}, where two nodes are related 
in the undirected network by how many commons nodes they point to in the 
directed network.  This relation called the {\sl bibliographic coupling} of two 
nodes.  In a brain network, this could give a \lq{}map\rq{} of information flow.

\section{The bibliographic network}
A directed network is defined as a collection of nodes and directed links between those nodes, represented by ordered pairs of nodes.  As mentioned towards the beginning of this chapter, the adjacency matrix $\mathbf{A}$ of a directed network is then defined by:
\begin{equation}
A_{ij} = \left\{ \begin{array}{ll} 1 & \text{if there is a link from $j$ to $i$} \\ 0 & \text{otherwise} \end{array}\right.
\end{equation}
The \emph{bibliographic coupling} of two nodes, $i$ and $j$, in an unweighted directed network is defined to be the number of common nodes that are pointed to by nodes $i$ and $j$.  Since an unweighted network typically has entries equal to one or zero, this coupling is calculated as:
\begin{equation}
B_{ij} = \sum_k A_{ki} A_{kj}
\end{equation}
Thus the matrix $\mathbf{B}$ of bibliographic couplings is: $\mathbf{B} = \mathbf{A}^T\mathbf{A}$, a common symmetrisation of a non-symmetric matrix.  The diagonal elements $B_{ii}$ are the number of nodes that node $i$ points to, or the \emph{out degree} of node $i$.

The bibliographic network can then be defined in a couple of ways.  It could be defined that each pair of nodes $i$ and $j$ with non-zero bibliographic coupling have a link, but this throws away a lot of information from the original directed network.  It is better to view the bibliographic network as a weighted network where each link $(i,j)$ has weight equal to the corresponding entry in the bibliographic matrix, $B_{ij}$, but without any self-links; so the bibliographic network has adjacency matrix equal to the bibliographic matrix with the diagonal entries set to zero.

\section{Incremental mutual information}
Incremental Mutual Information (IMI) is a measure defined by Singh and Lesica \citep{SinghLesica2010a} which is used to determine whether a pair of time-series have influence upon each other, for a given time-delay.  It is a similar measure to transfer entropy \citep{Schreiber2000a}, but it also uses future information to further eliminate any correlations due to noise.

Given two spike-trains $X$ and $Y$, a time $t$, and a time-delay $\delta$, the contribution to the IMI at time $t$ is defined to be:
\begin{equation}
\Delta I_{XY}[\delta] = H\left(X(t) | Z_{\delta}(t)\right) - H\left(X(t) | Z_{\delta}(t),Y(t-\delta)\right)
\end{equation}
where $Z_{\delta}$ is a vector containing information about the future and past of both spike-trains $X$ and $Y$, with a delay of $\delta$ added to $Y$:
\begin{equation}
Z_{\delta}(t) =  \left(X_p(t),X_f(t),Y_p(t-\delta), Y_f(t-\delta)\right)
\end{equation}

This is calculated in the typical manner of calculating mutual information between spike-trains introduced in \citep{BialekEtAl1998a}.  The spike-trains are discretised according to a time-scale which is unlikely to contain more than one spike, for example $2$ ms which is approximately the absolute refractory period of most neurons. That is, the spike trains are split into vectors of bins which either contain a spike or do not, and as such are allocated either one or zero.  The probability space is then calculated for each combination of ``words'' of ones and zeros.

The downside to this method is that this is a very data-hungry calculation, which limits the amount of `past' and `future' steps used, as for $\Omega$ past and future steps, there are $2^{4\Omega+2}$ possible states. This means that calculating the IMI with two steps forward and back at each time step requires on the order of $2^{15}$ data points to fully populate the sample space.

\section{Numerical testing}

\begin{figure}[h!tb]
  \centering
  \includegraphics[width=0.9\textwidth]{modelnetwork.eps}
  \bigskip
  \rule{31.5em}{0.5pt}
  \caption{\label{modelnetwork}The network model which was used to test the effectiveness of the incremental mutual information was based on the schematic shown here.  All simulated neurons in the recurrent layer were aEIF neurons.  The connection probability between two nodes at random was $0.1$, connections along arrows in the diagram occurred with probability $0.8$, and connections from the noise layer to the recurrent layer occurred with probability $0.2$.}
\end{figure}

IMI requires very long data sets to calculate accurately, which are difficult to record in animals that are awake. Due to the scarcity of  large simultaneous recordings for long periods of time, the proposed information mapping was instead carried out on a model network.  The network was designed to be small, for ease of calculations, but still had features of note - a divergence and a convergence of groups.  The network was randomly generated based on the schematic in Figure \ref{modelnetwork}.  The neuron-model used was the adaptive exponential integrate and fire neuron-model of \citep{BretteGerstner2005a}, as it strikes a good balance between ease of computation \citep{HopfieldHerz1995a}, and biological accuracy \citep{HodgkinHuxley1952a}.  The network was simulated using the python package Brian \citep{GoodmanBrette2008a}, and each simulation ran for four minutes.  The amplitude of the spikes from the Poisson neurons in the noise layer was varied from 1 mV to 17 mV to investigate how well the IMI dealt with noise.

\begin{figure}[h!]
\centering
\linespread{2}
\begin{tabular}{ll}
{\bf (a)} & {\bf (b)}\\
\epsfig{file=directednetnodes.eps,width=0.4\textwidth}
 & \resizebox{0.4\textwidth}{!}{\input{images/conn1}}
\end{tabular}
\bigskip
\rule{31.5em}{0.5pt}
\caption{\label{netwm} {\bf (a)} A typical visualisation of the directed network, very difficult to see what sort of modules exist, despite the layout. {\bf (b) } A heat map of the adjacency matrix of the recurrent layer of aEIF neurons, again there are very few clusters around the diagonal here, which would indicate a module in the network.}
\end{figure}

The proposed method to map the modules of neurons was to initially calculate the peak IMI between each pair of neurons, and set that equal to the non-diagonal elements of an adjacency matrix of a weighted directed network:
\begin{equation}
A_{ij} = \left\{ \begin{array}{ll} \max_{\delta} \Delta I_{ij}[\delta] & \text{if } \delta>0 \\ 0 & \text{otherwise} \end{array}\right.
\end{equation}


\begin{figure}[thb]
\begin{center}
\begin{tabular}{ll}
{\bf (i)} & {\bf (ii)} \\
\resizebox{0.45\textwidth}{!}{\input{images/IMI3}} & \resizebox{0.45\textwidth}{!}{\input{images/IMI7}}\\
{\bf (iii)} & {\bf (iv)} \\
\resizebox{0.45\textwidth}{!}{\input{images/IMI11}} & \resizebox{0.45\textwidth}{!}{\input{images/IMI15}} 
\end{tabular}
\bigskip
\rule{31.5em}{0.5pt}
\caption{\label{imires} Above are shown the peak IMI values between neurons, calculated from their spikes trains. The adjacency matrix is directed so that the peak IMI dictates the direction of influence of the neurons. The amplitude of the individual neurons in the Poisson noise layer increased at each step {\bf (i)} $3$ mV to {\bf (iv)}$15$ mV in steps of $4$ mV. Note that the IMI actually becomes more discriminating as the noise level increases.}
\end{center}
\end{figure}


The absolute value of the IMI was quite small, particularly as the noise level increased, but the important information was in the discriminability of the measure.  Therefore, the non-zero IMI values were collected and the median was calculated.  This median, $\tau_I$ was then set as the threshold for the network.  The new adjacency matrix was then calculated as:
\begin{equation}\label{adjmat}
A_{ij, \tau_I} = \left\{ \begin{array}{ll} 1 & \text{if } \max_{\delta} \Delta I_{ij}[\delta] > \tau_I \\ 0 &  \text{otherwise}. \end{array} \right.
\end{equation}


As can be seen in Figure \ref{imires}, the IMI actually appears to improve in discriminability as the noise level increases.  Accounting for noise was the reason for conditioning for the future along with the past \citep{SinghLesica2010a}, as opposed to just conditioning for the past as \citet{Schreiber2000a} does with the Transfer Entropy measure.  This means that the IMI could be a very good tool to measure influence between neurons in simultaneous recordings, but it does require a lot of data before it is useful, and it has a formidable calculation time.

The next step in the process is to calculate the bibliographic coupling from the thresholded adjacency matrix, $\mathbf{A}_{\tau_I}$, calculated in equation \ref{adjmat}.  This then gives a new network, which is undirected, and hence can be clustered using common methods such as \citep{NewmanGirvan2004a, Newman2006a}. 

\begin{figure}[thb]
\begin{center}
\begin{tabular}{ll}
{\bf (a)} & {\bf (b)}\\
\resizebox{0.45\textwidth}{!}{\input{images/TB1}} & \resizebox{0.45\textwidth}{!}{\input{images/TB17}}
\end{tabular}
\end{center}
\bigskip
\rule{31.5em}{0.5pt}
\caption{\label{bibt} In this figure the thresholded bibliographic network is shown from the IMI network when: {\bf (a)} the noise level is very low, that is 1 mV from each Poisson noise neuron, and {\bf (b)} the noise level is much higher, 17mV from each Poisson neuron. The modular structure of the network is showing a lot more when the noise level is high.}
\end{figure}

In figure \ref{bibt}, the bibliographic network is shown to roughly group the first three groups together, but the fourth and fifth are also clustered together.  This is because they both influence the sixth group, so they have high bibliographic coupling.  It appears that the bibliographic coupling can detect a divergence of groups in a network, but cannot detect a convergence.

In order to correctly cluster the convergent groups towards the end of the model network, another measure needs to be introduced, the cocitation.  This coupling is the inverse concept to the bibliographic coupling, as it is the number of common nodes pointing at both nodes.  That is:
\begin{equation}
C_{ij} = \sum_k A_{ik}A_{jk}
\end{equation}
The matrix is then $\mathbf{C} = \mathbf{BB}^T$.  This is another very common symmetrisation of matrices.

After running the whole process again with the cocitation instead of the bibliographic coupling, it is clear that the cocitation does indeed detect convergences in the groups of neurons, this can be seen in figure \ref{cocitt}.

\begin{figure}[thb]
\begin{center}
\begin{tabular}{ll}
{\bf (a)} & {\bf (b)}\\
\resizebox{0.45\textwidth}{!}{\input{images/TC1}} & \resizebox{0.45\textwidth}{!}{\input{images/TC17}}
\end{tabular}
\end{center}
\bigskip
\rule{31.5em}{0.5pt}
\caption{\label{cocitt} In this figure the thresholded cocitation network is shown from  the IMI network when: {\bf (a)} the noise level is very low, that is 1 mV from each Poisson noise neuron, and {\bf (b)} the noise level is much higher, 17mV from each Poisson neuron. The modular structure of the network is showing a lot more when the noise level is high.}
\end{figure}


\section{Discussion}
The incremental mutual information proved to be a remarkably useful tool in noisy networks, but it requires very large data sets to calculate, these can be very difficult to record in vivo.  The length of data sets required may rule out the IMI as a useful measure of real world data.

The bibliographic coupling and  the cocitation both gave interesting clusterings of the network, but combining them to consistently return a provided network was a difficult task, which was unfortunately not completed in the course of this work.

Perhaps attempting to map the information flow in a network is beyond current data capabilities, but there should be some consistent way to compare simultaneous recordings of many neurons.  Previous multi-unit spike train metrics have had both time-scale and population parameters which needed to be tuned. By extending the time-scale free distance measures developed by \citet{KreuzEtAl2007a,KreuzEtAl2012a}, this will reduce the need for at least one of the parameters.
