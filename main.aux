\relax 
\bibstyle{apalike}
\citation{DuBoisReymond1884a}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Spike trains}{1}}
\citation{GoodmanBrette2008a}
\citation{GoodmanBrette2008a}
\citation{HodgkinHuxley1952a}
\citation{HodgkinHuxley1939a}
\citation{Izhikevich2004a}
\citation{Lewicki1998a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The voltage trace of a pair of spikes generated by the Hodgkin-Huxley model. Simulation run using the python package Brian \cite  {GoodmanBrette2008a}}}{3}}
\citation{VanRossum2001a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Spike train metrics}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}van Rossum metric}{4}}
\citation{PaivaParkPrincipe2010}
\citation{VictorPurpura1997a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Shown above is an example of the filtering process of the van Rossum metric. A sample spike train (a) is convolved with an exponential filter (b), where the timescale $\tau =20$ ms, to get the function $u(t)$ (c).}}{5}}
\newlabel{stk}{{1.2}{5}}
\citation{KreuzEtAl2011a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Victor-Purpura metric}{6}}
\citation{BialekEtAl1998a}
\citation{GillespieHoughton2009a}
\citation{Shannon1948a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}SPIKE distance}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Information Theory}{7}}
\citation{Newman2010a}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Clustering methods in Spike Train Analysis}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Newman2006b}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Modularity}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The advantage to clustering a network correctly is seen here. Both of these networks have the same nodes and links, and so are the same network, but they look vastly different because the diagram on the left has been clustered to maximise modularity.}}{10}}
\newlabel{netclus}{{2.1}{10}}
\citation{BenderCanfield1978a}
\citation{Bollobas1980a}
\citation{Newman2010a}
\newlabel{probki}{{2.4}{11}}
\citation{Newman2006a}
\newlabel{NewMod}{{2.8}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Newman's eigenvalue algorithm}{12}}
\newlabel{ssMod}{{2.11}{13}}
\citation{Zachary1977a}
\citation{Newman2006a}
\citation{Newman2006a}
\citation{Newman2006a}
\citation{NewmanGirvan2004a}
\citation{Humphries2011a}
\citation{Newman2006a}
\citation{NewmanGirvan2004a}
\citation{Pizzuti2008a}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}New network clustering algorithms}{16}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces This is Newman's eigenvalue algorithm for maximising the modularity of a network.}}{17}}
\newlabel{algo-split}{{1}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Genetic algorithm}{18}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces An example of a generic genetic algorithm.}}{19}}
\newlabel{genal}{{2}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Simulated annealing}{20}}
\citation{Zachary1977a}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}$k$-medoids clustering}{21}}
\citation{JulienneHoughton2012a}
\citation{NarayanEtAl2006b}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Clustering responses with modularity}{23}}
\citation{Humphries2011a}
\citation{SinghLesica2010a}
\citation{Newman2010a}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Mapping information flow in a simulated network of neurons}{25}}
\citation{SinghLesica2010a}
\citation{Schreiber2000a}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The bibliographic network}{26}}
\citation{BialekEtAl1998a}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Incremental mutual information}{27}}
\citation{BretteGerstner2005a}
\citation{HopfieldHerz1995a}
\citation{HodgkinHuxley1952a}
\citation{GoodmanBrette2008a}
\citation{SinghLesica2010a}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Numerical testing}{28}}
\newlabel{adjmat}{{2.25}{28}}
\citation{Newman2006a}
\citation{NewmanGirvan2004a}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Bibliographic coupling}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example of the first step in k-medoids clustering: (a) Medoids are initially selected at random and the first clustering puts each point in the cluster of the closest medoid. Then we choose the point in the cluster that will give the lowest total distance to other points, and choose that as the new medoid. (b) Finally, we re-cluster each point to go in the cluster of the closest medoid to it.}}{30}}
\newlabel{kmed}{{2.2}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The network model was based on the schematic shown here. All simulated neurons in the recurrent layer were aEIF neurons. The connection probability between two nodes at random was $0.1$, connections along arrows in the diagram occurred with probability $0.8$, and connections from the noise layer to the recurrent layer occurred with probability $0.2$.}}{31}}
\newlabel{modelnetwork}{{2.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  A standard representation of the directed network above, and the connection matrix of the recurrent layer of aEIF neurons below.}}{32}}
\newlabel{netwm}{{2.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  The connection matrix calculated from the peak IMI of the neurons, as the amplitude of the individual neurons in the noise layer increased from $1$ mV to $16$ mV. Note that the IMI actually becomes more discriminating as the noise level increases.}}{33}}
\newlabel{imires}{{2.5}{33}}
\citation{VanRossum2001a}
\citation{VictorPurpura1996a}
\citation{KreuzEtAl2007a}
\citation{KreuzEtAl2011a}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Spike Train Metrics}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{KreuzEtAl2011a}
\citation{KreuzEtAl2011a}
\citation{KreuzEtAl2011a}
\citation{KreuzEtAl2011a}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Single-unit recordings}{35}}
\citation{KreuzEtAl2011a}
\citation{VictorPurpura1997}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Extension to multi-unit recordings}{37}}
\citation{HoughtonSen2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example of how to calculate the gaps between spikes. If the circles represent spikes from $\mathbf  {X}$ and crosses represent spikes from $\mathbf  {Y}$, then in the picture above, the distance $k$ is simply the cost of relabelling a spike. As seen here, the gaps are not necessarily symmetric.}}{39}}
\newlabel{fig:gaps2}{{3.1}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Testing on data}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}ISI distance}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Single unit recordings}{42}}
\citation{HoughtonSen2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Initial extension to multi-unit recordings}{43}}
\citation{HoughtonSen2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Numerical tests}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The top line corresponds to the maximum possible transmitted information of $\qopname  \relax o{log}(5)$. Then the upper of the two remaining lines is the average of the maximum transmitted information as $\alpha $ is varied from zero to a half, and the bottom line is the minimum transmitted information over $\alpha $.}}{46}}
\newlabel{mmav}{{3.2}{46}}
\citation{Kreuzetal2009}
\citation{Kreuzetal2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Alternative extensions to the multi-unit case}{47}}
\newlabel{pop}{{3.23}{47}}
\newlabel{av}{{3.24}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The lines, as in figure 3.2\hbox {} correspond to the maximum and the minimum for: $s_p$, the `{}population'{} extension, in the solid lines; and $s_a$, the `{}average'{} extension, in the broken lines.}}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces This figure shows the optimal value for the population parameter $p$ for $s_a$, the `{}average'{} extension, as the mixing parameter $\alpha $ is varied from zero to a half, with the errorbars showing the standard deviation.}}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Adaptive ISI distances}{49}}
\citation{MulanskyEtAl2015a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces This figure shows the optimal value for the population parameter $p$ for $s_p$, the `{}population'{} extension, as the mixing parameter $\alpha $ is varied from zero to a half, with the errorbars showing the standard deviation.}}{50}}
\citation{OlshausenField2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A simple neuron model}{53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{lam}{{4.1}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  An illustration of the model firing rate, and an associated output spike train. The process has two states, an up-state and a down-state, and has Poisson transition rates: $u$ from down to up, and $d$ from up to down. Each state has an associated Poisson firing rate: $\lambda _u$ when up and $\lambda _d$ when down. }}{54}}
\newlabel{model}{{4.1}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Estimating the firing rate $r(t)$}{54}}
\citation{vanRossum2001}
\newlabel{p}{{4.2}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  This figure shows the ratio of the predicted value for $\tau $ in an exponential model over the empirical optimised value $\tau _o$.}}{57}}
\newlabel{predovertauopt}{{4.2}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Markov Process}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  An illustration of the state space of the Markov chain at any point in time between two spikes.}}{59}}
\newlabel{markov}{{4.3}{59}}
\newlabel{dg}{{4.23}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Estimating the rate function $r(t)$}{61}}
\newlabel{roft}{{4.28}{62}}
\newlabel{abcd}{{4.29}{62}}
\newlabel{rate}{{4.35}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Change in probability when a spike arrives}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Calculating the ISI distribution}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An example of the estimated rate, $r(t)$, for a bimodal Poisson spike train. For this example, $u=d=5$, $\lambda _u=50$ and $\lambda _d=5$. In this example, the initial rate was set randomly between $\alpha $ and $\beta $.}}{65}}
\newlabel{int}{{4.44}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces An example of the estimated rate, $r(t)$, for a bimodal Poisson spike train. For this example, $u=d=5$, $\lambda _u=50$ and $\lambda _d=5$. In this example, the initial rate was set to zero and it subsequently set itself.}}{66}}
\citation{NarayanEtAl2006b}
\citation{OlshausenField2004a}
\citation{BerryMeister1998a}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Testing on data}{67}}
\citation{Massey1951a}
\citation{Massey1951a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces  The Kolmogorov-Smirnov test statistic $D_n$ for (a) the exponential distribution, (b) the hyperexponential distribution with two modes, and (c) the hyperexponential distribution with three modes. The mean of the $p<0.05$ critical value is indicated by the broken line.}}{69}}
\newlabel{exphehe3}{{4.6}{69}}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{71}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{BenderCanfield1978a}{Bender and Canfield, 1978}
\bibcite{Bollobas1980a}{Bollob{\'a}s, 1980}
\bibcite{BretteGerstner2005a}{Brette and Gerstner, 2005}
\bibcite{DuBoisReymond1884a}{Du\nobreakspace  {}Bois-Reymond, 1884}
\bibcite{GillespieHoughton2009a}{Gillespie and Houghton, 2011}
\bibcite{GoodmanBrette2008a}{Goodman and Brette, 2008}
\bibcite{HodgkinHuxley1952a}{Hodgkin and Huxley, 1952}
\bibcite{HodgkinHuxley1939a}{Hodgkin and Huxley, 1939}
\bibcite{HopfieldHerz1995a}{Hopfield and Herz, 1995}
\bibcite{HoughtonSen2008}{Houghton and Sen, 2008}
\bibcite{Humphries2011a}{Humphries, 2011}
\bibcite{BerryMeister1998a}{II and Meister, 1998}
\bibcite{Izhikevich2004a}{Izhikevich, 2004}
\bibcite{JulienneHoughton2012a}{Julienne and Houghton, 2013}
\bibcite{Kreuzetal2009}{Kreuz et\nobreakspace  {}al., 2009}
\bibcite{KreuzEtAl2011a}{Kreuz et\nobreakspace  {}al., 2011}
\bibcite{KreuzEtAl2007a}{Kreuz et\nobreakspace  {}al., 2007}
\bibcite{Lewicki1998a}{Lewicki, 1998}
\bibcite{Massey1951a}{Massey\nobreakspace  {}Jr, 1951}
\bibcite{NarayanEtAl2006b}{Narayan et\nobreakspace  {}al., 2006}
\bibcite{Newman2006b}{Newman, 2006a}
\bibcite{Newman2006a}{Newman, 2006b}
\bibcite{Newman2010a}{Newman, 2010}
\bibcite{NewmanGirvan2004a}{Newman and Girvan, 2004}
\bibcite{OlshausenField2004}{Olshausen and Field, 2004a}
\bibcite{OlshausenField2004a}{Olshausen and Field, 2004b}
\bibcite{PaivaParkPrincipe2010}{Paiva et\nobreakspace  {}al., 2010}
\bibcite{Pizzuti2008a}{Pizzuti, 2008}
\bibcite{Schreiber2000a}{Schreiber, 2000}
\bibcite{Shannon1948a}{Shannon, 1948}
\bibcite{SinghLesica2010a}{Singh and Lesica, 2010}
\bibcite{BialekEtAl1998a}{Strong et\nobreakspace  {}al., 1998}
\bibcite{VanRossum2001a}{van Rossum, 2001a}
\bibcite{vanRossum2001}{van Rossum, 2001b}
\bibcite{VictorPurpura1996a}{Victor and Purpura, 1996}
\bibcite{VictorPurpura1997a}{Victor and Purpura, 1997a}
\bibcite{VictorPurpura1997}{Victor and Purpura, 1997b}
\bibcite{Zachary1977a}{Zachary, 1977}
