\relax 
\bibstyle{apalike}
\citation{NarayanEtAl2006b}
\citation{HoughtonSen2008a}
\citation{Newman2006b}
\citation{SinghLesica2010a}
\citation{KreuzEtAl2012a}
\citation{KreuzEtAl2007a}
\citation{Massey1951a}
\citation{Knight1972a}
\citation{HopkinsBass1981a,EngelEtAl1992a}
\citation{Cajal1904a}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Cajal1904a}
\citation{Cajal1904a}
\citation{DuBoisReymond1884a}
\citation{GoodmanBrette2008a}
\citation{GoodmanBrette2008a}
\citation{HodgkinHuxley1952a}
\citation{HodgkinHuxley1939a}
\citation{Izhikevich2004a}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Spike trains}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Diagrams of neurons as drawn by \citet  {Cajal1904a}. (a) depicts a cortical pyramidal cell . Pyramidal cells are the primary excitatory neurons of the cerebral cortex. Pyramidal cells are thus often modelled as a typical neuron. (b) depicts a Purkinje cell of a pigeon. Purkinje cells are found in the cerebellum in the human brain. They are some of the largest cells in the human brain. With their dense branching dendrites they have much higher connectivity than pyramidal cells.}}{3}}
\newlabel{Cajal}{{1.1}{3}}
\citation{Lewicki1998a}
\citation{BiPoo1998a,Bair1999a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The voltage trace of a pair of spikes generated by the Hodgkin-Huxley model. Simulation run using the python package Brian \citep  {GoodmanBrette2008a}}}{4}}
\citation{AverbeckEtAl2006a}
\citation{Hopfield1982a}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Spike train metrics}{5}}
\citation{VictorPurpura1997a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Victor-Purpura metric}{6}}
\citation{VanRossum2001a}
\citation{VanRossum2001a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}van Rossum metric}{7}}
\citation{HoughtonKreuz2012a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces  Shown above is an example of the filtering process of the van Rossum metric. {\bf  (a)} A sample spike train is convolved with {\bf  (b)} an exponential filter, where the timescale $\tau =20$ ms, to get {\bf  (c)} the function $u(t)$ .}}{8}}
\newlabel{stk}{{1.3}{8}}
\citation{HoughtonVictor2009a}
\citation{KreuzEtAl2007a}
\citation{KreuzEtAl2007a,KreuzEtAl2009a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}ISI distance}{9}}
\citation{KreuzEtAl2011a,KreuzEtAl2012a}
\citation{VictorPurpura1997a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces  Shown here is an example pair of spike trains $X$ and $Y$. At each point $t$ in the trial there are respective intervals $I_x(t)$ and $I_y(t)$. By taking the reciprocal of $I_x(t)$ and $I_y(t)$, an estimator for the firing rate of $X$ and $Y$ at time $t$ is calculated.}}{10}}
\newlabel{isiex}{{1.4}{10}}
\citation{KreuzEtAl2012a}
\citation{KreuzEtAl2012a}
\citation{KreuzEtAl2009a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}SPIKE distance}{11}}
\citation{BialekEtAl1998a}
\citation{GillespieHoughton2009a}
\citation{SinghLesica2010a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces  Shown here is the same pair of spike trains $X$ and $Y$ as in Figure 1.4\hbox {}, but here the measures relevant for the SPIKE distance \citep  {KreuzEtAl2012a} are highlighted. At each point $t$ in the trial there is a preceding and a following spike in each spike train, called $t^x_p, t^x_f, t^y_p, t^y_f$ respectively. At each of these, so called, `{}corner'{} spikes a `{}gap'{} is calculated. The gap for each spike is the shortest distance to a spike in the other spike train.}}{12}}
\newlabel{spikeex}{{1.5}{12}}
\citation{Shannon1948a}
\citation{BialekEtAl1998a}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Information Theory}{13}}
\citation{BialekEtAl1998a}
\citation{BialekEtAl1998a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces The method developed in \citep  {BialekEtAl1998a} for calculating the entropy and mutual information of spike trains. The spike train is sampled at a rate $\Delta \tau $, typically on the order of $2-5$ ms. The spike train is binned, and each time-step registers only whether there is a spike in that bin or not, putting a value of one or zero in the bin accordingly. On another, larger time-scale, $T$, words of ones and zeros of length $(T/\Delta \tau )$ are counted. These words form the probability distribution for the entropy.}}{14}}
\citation{NarayanEtAl2006b}
\citation{NarayanEtAl2006b}
\citation{NarayanEtAl2006b}
\citation{SenEtAl2001a}
\newlabel{info}{{1.21}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Data sets}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Zebra finch data}{15}}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces The data used was collected for \citep  {NarayanEtAl2006b}. A total of 20 songs were played to anaesthetised zebra finches, and the responses were recorded in the auditory forebrain. Each song was presented ten times. Responses for three different cells are shown here.}}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Multi-unit data}{16}}
\citation{VictorPurpura1996a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces This is the schematic for the data set used to test the multi-unit metrics in this thesis. It is the same data set that was used in \citep  {HoughtonSen2008a}. The receptive field neurons fire Poisson spike trains according to the rate function input they receive. These Poisson spike trains feed into the leaky integrate and fire neurons from which the response is measured. The synaptic strengths are parametrised by $a$. When $a$ is zero, the neurons receive independent input; when $a=0.5$, the input should be completely mixed. }}{18}}
\citation{Newman2010a}
\citation{Zachary1977a}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Clustering methods in spike train analysis}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{NewmanGirvan2004a}
\citation{NewmanGirvan2004a}
\citation{Newman2006b,Newman2006a}
\citation{BenderCanfield1978a,Bollobas1980a}
\citation{Newman2006b,Newman2006a}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Modularity}{20}}
\citation{Newman2006b,Newman2006a}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The advantage to clustering a network correctly is seen here. Both of these networks have the same nodes and links, and so are the same network, but they look vastly different because the diagram on the left has been clustered to maximise modularity.}}{21}}
\newlabel{netclus}{{2.1}{21}}
\citation{BenderCanfield1978a,Bollobas1980a}
\citation{Newman2010a}
\newlabel{probki}{{2.5}{22}}
\citation{Newman2006a,Newman2006b}
\newlabel{NewMod}{{2.9}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Newman's eigenvalue algorithm}{23}}
\citation{Zachary1977a}
\citation{Newman2006a}
\newlabel{ssMod}{{2.12}{24}}
\citation{Newman2006a}
\citation{Newman2006a}
\citation{Humphries2011a}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces This is Newman's eigenvalue algorithm for maximising the modularity of a network.}}{27}}
\newlabel{algo-split}{{1}{27}}
\citation{Newman2006a}
\citation{NewmanGirvan2004a}
\citation{Pizzuti2008a}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}New network clustering algorithms}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Genetic algorithm}{28}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces An example of a generic genetic algorithm.}}{29}}
\newlabel{genal}{{2}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Simulated annealing}{30}}
\citation{Zachary1977a}
\citation{JulienneHoughton2012a}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}$k$-medoids clustering}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example of the first step in k-medoids clustering: (a) Medoids are initially selected at random and the first clustering puts each point in the cluster of the closest medoid. Then we choose the point in the cluster that will give the lowest total distance to other points, and choose that as the new medoid. (b) Finally, we re-cluster each point to go in the cluster of the closest medoid to it. }}{32}}
\newlabel{kmed}{{2.2}{32}}
\citation{JulienneHoughton2012a}
\citation{NarayanEtAl2006b}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Clustering responses with modularity}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Above is the graph of the modularity versus threshold values in two typical cells from the data. The green crosses represent the modularity of the correct clustering of stimuli; the red plusses represent the maximum modularity, according to the algorithm, of the network. Despite there being clear peaks in the modularity graphs, the green consistently outperforms the red for every value of the threshold $\tau $. This seems to imply that network clustering methods are probably not very good for sorting stimuli. }}{34}}
\newlabel{graphs}{{2.3}{34}}
\citation{Humphries2011a}
\citation{Newman2006b}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Mapping information flow in a simulated network of neurons}{35}}
\citation{SinghLesica2010a}
\citation{Newman2010a}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The bibliographic network}{36}}
\citation{SinghLesica2010a}
\citation{Schreiber2000a}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Incremental mutual information}{37}}
\citation{BialekEtAl1998a}
\citation{BretteGerstner2005a}
\citation{HopfieldHerz1995a}
\citation{HodgkinHuxley1952a}
\citation{GoodmanBrette2008a}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Numerical testing}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The network model which was used to test the effectiveness of the incremental mutual information was based on the schematic shown here. All simulated neurons in the recurrent layer were aEIF neurons. The connection probability between two nodes at random was $0.1$, connections along arrows in the diagram occurred with probability $0.8$, and connections from the noise layer to the recurrent layer occurred with probability $0.2$.}}{39}}
\newlabel{modelnetwork}{{2.4}{39}}
\citation{SinghLesica2010a}
\citation{Schreiber2000a}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  {\bf  (a)} A typical visualisation of the directed network, very difficult to see what sort of modules exist, despite the layout. {\bf  (b) } A heat map of the adjacency matrix of the recurrent layer of aEIF neurons, again there is are very few clusters around the diagonal here, which would indicate a module in the network.}}{40}}
\newlabel{netwm}{{2.5}{40}}
\newlabel{adjmat}{{2.27}{40}}
\citation{Newman2006a,NewmanGirvan2004a}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Discussion}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Above are shown the peak IMI values between neurons, calculated from their spikes trains. The adjacency matrix is directed so that the peak IMI dictates the direction of influence of the neurons. The amplitude of the individual neurons in the Poisson noise layer increased from {\bf  (a)} $1$ mV to {\bf  (ix)}$16$ mV in steps of $2$ mV. Note that the IMI actually becomes more discriminating as the noise level increases.}}{43}}
\newlabel{imires}{{2.6}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  In this figure the thresholded bibliographic network is shown from the IMI network when: {\bf  (a)} the noise level is very low, that is 1 mV from each Poisson noise neuron, and {\bf  (b)} the noise level is much higher, 17mV from each Poisson neuron. The modular structure of the network is showing a lot more when the noise level is high.}}{44}}
\newlabel{bibt}{{2.7}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  In this figure the thresholded cocitation network is shown from the IMI network when: {\bf  (a)} the noise level is very low, that is 1 mV from each Poisson noise neuron, and {\bf  (b)} the noise level is much higher, 17mV from each Poisson neuron. The modular structure of the network is showing a lot more when the noise level is high.}}{44}}
\newlabel{cocitt}{{2.8}{44}}
\citation{KreuzEtAl2011a,KreuzEtAl2012a}
\citation{KreuzEtAl2011a}
\citation{KreuzEtAl2011a}
\citation{KreuzEtAl2011a}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Multi-unit spike train metrics}{45}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Alternative SPIKE distance}{45}}
\citation{KreuzEtAl2011a}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Extensions to multi-unit recordings}{47}}
\citation{HoughtonSen2008a}
\citation{Aronovetal2003a}
\citation{VictorPurpura1997a}
\newlabel{initspike}{{3.7}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example of how to calculate the gaps between spikes. If the circles represent spikes from $\mathbf  {X}$ and crosses represent spikes from $\mathbf  {Y}$, then in the picture above, the distance $k$ is simply the cost of relabelling a spike. Spike {\bf  e} in $Y_2$ is much closer to spike {\bf  a} in $X_1$ than {\bf  f} in $X_2$, so it pays the cost $k$ to get its $\Delta $ from {\bf  a}.}}{49}}
\newlabel{fig:gaps2}{{3.1}{49}}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\citation{HoughtonSen2008a}
\newlabel{multspike}{{3.13}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Testing on data}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces This figure shows the performance of the multi-unit SPIKE extension in identifying stimuli correctly in the data set described above. The plot shows the maximum and minimum values of the transmitted information for each value of the mixing parameter $a$, averaged over 20 trials. This compares favourably with Figure 3 in \citep  {HoughtonSen2008a}.}}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}ISI distance}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Single unit recordings}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces This figure shows the value for the penalty cost $k$ for the multi-unit SPIKE distance which gave the maximum transmitted information for each value of $a$. There does not appear to be any trend towards LL or SP in either direction, the best value was always a mix of the two.}}{53}}
\citation{HoughtonSen2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces This figure shows the performance of the multiplicative multi-unit SPIKE extension, using the gaps from equation 3.13\hbox {}, in identifying stimuli correctly in the data set described above. The plot shows the maximum and minimum values of the transmitted information for each value of the mixing parameter $a$, averaged over 20 trials.}}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Initial extension to multi-unit recordings}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces This figure shows the value for the population parameter $p$ for the multiplicative multi-unit SPIKE distance which gave the maximum transmitted information for each value of $a$. There does not appear to be any trend towards LL or SP in either direction.}}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The top line corresponds to the maximum possible transmitted information of $\qopname  \relax o{log}(5)$. Then the upper of the two remaining lines is the average of the maximum transmitted information as $\alpha $ is varied from zero to a half, and the bottom line is the minimum transmitted information over $\alpha $.}}{57}}
\newlabel{mmav}{{3.6}{57}}
\citation{KreuzEtAl2009a}
\citation{KreuzEtAl2009a}
\citation{KreuzEtAl2009a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Numerical tests}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Alternative extensions to the multi-unit case}{58}}
\newlabel{pop}{{3.25}{58}}
\citation{KreuzEtAl2009a}
\newlabel{av}{{3.26}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The lines, as in figure 3.6\hbox {} correspond to the maximum and the minimum for: $s_p$, the `{}population'{} extension, in the solid lines; and $s_a$, the `{}average'{} extension, in the broken lines.}}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces This figure shows the optimal value for the population parameter $p$ for $s_a$, the `{}average'{} extension, as the mixing parameter $a$ is varied from zero to a half, with the errorbars showing the standard deviation.}}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces This figure shows the optimal value for the population parameter $p$ for $s_p$, the `{}population'{} extension, as the mixing parameter $a$ is varied from zero to a half, with the errorbars showing the standard deviation.}}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Adaptive ISI distances}{61}}
\citation{MulanskyEtAl2015a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces   Above is the performance of the adaptive multi-unit ISI metrics. (a) $s_a$ with population parameter $p$ at each instant set to one if $s(t)<0.5$ and set to zero otherwise. (b) $s_p$ with population parameter $p$ at each instant set to one if $s(t)<0.5$ and set to zero otherwise. (c) $s_a$ with population parameter $p$ at each instant set to 0.5 if $s(t)<0.5$ and set to zero otherwise. (d) $s_p$ with population parameter $p$ at each instant set t0 0.5 if $s(t)<0.5$ and set to zero otherwise.}}{63}}
\newlabel{zeroone}{{3.10}{63}}
\citation{OlshausenField2004a}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A simple neuron model}{64}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{lam}{{4.1}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  An illustration of the model firing rate, and an associated output spike train. The process has two states, an up-state and a down-state, and has Poisson transition rates: $u$ from down to up, and $d$ from up to down. Each state has an associated Poisson firing rate: $\lambda _u$ when up and $\lambda _d$ when down. }}{65}}
\newlabel{model}{{4.1}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Estimating the firing rate $r(t)$}{65}}
\citation{VanRossum2001a}
\newlabel{p}{{4.2}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  This figure shows the ratio of the predicted value for $\tau $ in an exponential model over the empirical optimised value $\tau _o$.}}{67}}
\newlabel{predovertauopt}{{4.2}{67}}
\citation{NelderMead1965a}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Markov Process}{68}}
\citation{KijimaKomoribayashi1998a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  An illustration of the state space of the Markov chain at any point in time between two spikes.}}{70}}
\newlabel{markov}{{4.3}{70}}
\newlabel{dg}{{4.26}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Estimating the rate function $r(t)$}{72}}
\newlabel{roft}{{4.31}{72}}
\newlabel{abcd}{{4.32}{72}}
\newlabel{rate}{{4.38}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Change in probability when a spike arrives}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Calculating the ISI distribution}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An example of the estimated rate, $r(t)$, for a bimodal Poisson spike train. For this example, $u=d=5$, $\lambda _u=50$ and $\lambda _d=5$. In this example, the initial rate was set randomly between $\alpha $ and $\beta $.}}{75}}
\newlabel{int}{{4.47}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces An example of the estimated rate, $r(t)$, for a bimodal Poisson spike train. For this example, $u=d=5$, $\lambda _u=50$ and $\lambda _d=5$. In this example, the initial rate was set to zero and it subsequently set itself.}}{76}}
\citation{NarayanEtAl2006b}
\citation{OlshausenField2004a}
\citation{BerryMeister1998a}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Testing on data}{77}}
\citation{Massey1951a}
\citation{Massey1951a}
\citation{Kolmogorov1933a}
\citation{Smirnoff1939a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces  The Kolmogorov-Smirnov test statistic $D_n$ for (a) the exponential distribution, (b) the hyperexponential distribution with two modes, and (c) the hyperexponential distribution with three modes. The mean of the $p<0.05$ critical value is indicated by the broken line.}}{78}}
\newlabel{exphehe3}{{4.6}{78}}
\citation{SinghLesica2010a}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{80}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{KreuzEtAl2007a,KreuzEtAl2012a}
\citation{AronovEtAl2008a}
\citation{HoughtonSen2008a}
\bibdata{bibliography}
\bibcite{AronovEtAl2008a}{{1}{2008}{{Aronov et~al.}}{{}}}
\bibcite{Aronovetal2003a}{{2}{2003}{{Aronov et~al.}}{{}}}
\bibcite{AverbeckEtAl2006a}{{3}{2006}{{Averbeck et~al.}}{{}}}
\bibcite{Bair1999a}{{4}{1999}{{Bair}}{{}}}
\bibcite{BenderCanfield1978a}{{5}{1978}{{Bender and Canfield}}{{}}}
\bibcite{BerryMeister1998a}{{6}{1998}{{Berry and Meister}}{{}}}
\bibcite{BiPoo1998a}{{7}{1998}{{Bi and Poo}}{{}}}
\bibcite{Bollobas1980a}{{8}{1980}{{Bollob{\'a}s}}{{}}}
\bibcite{BretteGerstner2005a}{{9}{2005}{{Brette and Gerstner}}{{}}}
\bibcite{DuBoisReymond1884a}{{10}{1884}{{Du~Bois-Reymond}}{{}}}
\bibcite{EngelEtAl1992a}{{11}{1992}{{Engel et~al.}}{{}}}
\bibcite{GillespieHoughton2009a}{{12}{2011}{{Gillespie and Houghton}}{{}}}
\bibcite{GoodmanBrette2008a}{{13}{2008}{{Goodman and Brette}}{{}}}
\bibcite{HodgkinHuxley1952a}{{14}{1952}{{Hodgkin and Huxley}}{{}}}
\bibcite{HodgkinHuxley1939a}{{15}{1939}{{Hodgkin and Huxley}}{{}}}
\bibcite{HopfieldHerz1995a}{{16}{1995}{{Hopfield and Herz}}{{}}}
\bibcite{Hopfield1982a}{{17}{1982}{{Hopfield}}{{}}}
\bibcite{HopkinsBass1981a}{{18}{1981}{{Hopkins and Bass}}{{}}}
\bibcite{HoughtonKreuz2012a}{{19}{2012}{{Houghton and Kreuz}}{{}}}
\bibcite{HoughtonSen2008a}{{20}{2008a}{{Houghton and Sen}}{{}}}
\bibcite{HoughtonSen2008}{{21}{2008b}{{Houghton and Sen}}{{}}}
\bibcite{HoughtonVictor2009a}{{22}{2012}{{Houghton and Victor}}{{}}}
\bibcite{Humphries2011a}{{23}{2011}{{Humphries}}{{}}}
\bibcite{Izhikevich2004a}{{24}{2004}{{Izhikevich}}{{}}}
\bibcite{JulienneHoughton2012a}{{25}{2013}{{Julienne and Houghton}}{{}}}
\bibcite{KijimaKomoribayashi1998a}{{26}{1998}{{Kijima and Komoribayashi}}{{}}}
\bibcite{Knight1972a}{{27}{1972}{{Knight}}{{}}}
\bibcite{Kolmogorov1933a}{{28}{1933}{{Kolmogorov}}{{}}}
\bibcite{KreuzEtAl2009a}{{29}{2009}{{Kreuz et~al.}}{{}}}
\bibcite{KreuzEtAl2011a}{{30}{2011}{{Kreuz et~al.}}{{}}}
\bibcite{KreuzEtAl2012a}{{31}{2013}{{Kreuz et~al.}}{{}}}
\bibcite{KreuzEtAl2007a}{{32}{2007}{{Kreuz et~al.}}{{}}}
\bibcite{Lewicki1998a}{{33}{1998}{{Lewicki}}{{}}}
\bibcite{Massey1951a}{{34}{1951}{{Massey~Jr}}{{}}}
\bibcite{MulanskyEtAl2015a}{{35}{2015}{{Mulansky et~al.}}{{}}}
\bibcite{NarayanEtAl2006b}{{36}{2006}{{Narayan et~al.}}{{}}}
\bibcite{NelderMead1965a}{{37}{1965}{{Nelder and Mead}}{{}}}
\bibcite{Newman2006b}{{38}{2006a}{{Newman}}{{}}}
\bibcite{Newman2006a}{{39}{2006b}{{Newman}}{{}}}
\bibcite{Newman2010a}{{40}{2010}{{Newman}}{{}}}
\bibcite{NewmanGirvan2004a}{{41}{2004}{{Newman and Girvan}}{{}}}
\bibcite{OlshausenField2004a}{{42}{2004}{{Olshausen and Field}}{{}}}
\bibcite{Pizzuti2008a}{{43}{2008}{{Pizzuti}}{{}}}
\bibcite{Cajal1904a}{{44}{1904}{{Ram{\'o}n~y Cajal}}{{}}}
\bibcite{Schreiber2000a}{{45}{2000}{{Schreiber}}{{}}}
\bibcite{SenEtAl2001a}{{46}{2001}{{Sen et~al.}}{{}}}
\bibcite{Shannon1948a}{{47}{1948}{{Shannon}}{{}}}
\bibcite{SinghLesica2010a}{{48}{2010}{{Singh and Lesica}}{{}}}
\bibcite{Smirnoff1939a}{{49}{1939}{{Smirnoff}}{{}}}
\bibcite{BialekEtAl1998a}{{50}{1998}{{Strong et~al.}}{{}}}
\bibcite{VanRossum2001a}{{51}{2001}{{van Rossum}}{{}}}
\bibcite{VictorPurpura1996a}{{52}{1996}{{Victor and Purpura}}{{}}}
\bibcite{VictorPurpura1997a}{{53}{1997}{{Victor and Purpura}}{{}}}
\bibcite{Zachary1977a}{{54}{1977}{{Zachary}}{{}}}
